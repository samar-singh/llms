{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Document loading, retrieval methods and text splitting\n",
    "# !pip install -qU langchain langchain_community\n",
    "\n",
    "# # Local vector store via Chroma\n",
    "# !pip install -qU langchain_chroma\n",
    "\n",
    "# # Local inference and embeddings via Ollama\n",
    "# !pip install -qU langchain_ollama\n",
    "\n",
    "# # Web Loader\n",
    "# !pip install -qU beautifulsoup4\n",
    "\n",
    "# # Pull the model first\n",
    "# !ollama pull nomic-embed-text\n",
    "\n",
    "# !pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\2020HT80616.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\2020HT80616_RCC_Lab_Assignment_1.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\2020HT80616_RCC_Lab_Assignment_1.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\2020HT80616_RCC_Lab_Assignment_2.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\2020HT80616_RCC_Lab_Assignment_2.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\Abstract_2020ht80616.pdf\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master.zip\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\FINAL SEM REPORT_2020HT80616.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\MID SEM REPORT_2020HT80616.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\MID SEM REPORT_2020HT80616.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\SampleProjectReport.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\VLSI_Architecture_Assignment_2_ALU.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\VLSI_Architecture_Assignment_2_ALU.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\VLSI_Architecture_Assignment_3_RISC_DESIGN.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\VLSI_Architecture_Assignment_3_RISC_DESIGN.pdf\n",
      "✗ Error loading ../WILP_ASSIGNMENT_WORK\\~$SI_Architecture_Assignment_3_RISC_DESIGN.docx: File is not a zip file\n",
      "✗ Error loading ../WILP_ASSIGNMENT_WORK\\~WRL2717.tmp: Error loading ../WILP_ASSIGNMENT_WORK\\~WRL2717.tmp\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\adder.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\alu.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\comparator.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\controller.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\decodePipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\executePipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\insMemory.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\pc.v\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\pipeline.png\n",
      "✗ Error loading ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\README.md: unstructured package not found, please install it with `pip install unstructured`\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\registers.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\riscTop.v\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\risc_design.png\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\shifter.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\signExtend.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\tb_risc.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master\\cpu_risc-master\\writebackPipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\adder.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\alu.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\comparator.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\controller.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\decodePipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\executePipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\insMemory.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\pc.v\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\pipeline.png\n",
      "✗ Error loading ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\README.md: unstructured package not found, please install it with `pip install unstructured`\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\registers.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\riscTop.v\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\risc_design.png\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\shifter.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\signExtend.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\tb_risc.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\cpu_risc-master_default\\cpu_risc-master\\writebackPipe.v\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\uart_led_lab\\MID SEM REPORT_2020HT80616.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\uart_led_lab\\Sources\\SDK_zed\\uart_through_gpio.sdk\\.metadata\\.lock\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\uart_led_lab\\Sources\\SDK_zybo\\uart_through_gpio.sdk\\.metadata\\.lock\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\BITSpresentation.ppt\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\BITSpresentation_org.ppt\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\FINAL REPORT_2020HT80616_Pre-Final.docx\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\Final_Report_2020ht80616.pdf\n",
      "✗ Error loading ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\~WRL2199.tmp: Error loading ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\~WRL2199.tmp\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\From Mac\\bitsfinalreportdocs.zip\n",
      "⚠ Skipping unsupported file: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\From Mac\\bitsfinalreportdocs\\2020HT80616.pptx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 220 0 (offset 0)\n",
      "Ignoring wrong pointing object 225 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\From Mac\\bitsfinalreportdocs\\2020HT80616FR.pdf\n",
      "✓ Successfully loaded: ../WILP_ASSIGNMENT_WORK\\WILP Final Dissertation\\From Mac\\bitsfinalreportdocs\\2020HT80616ppt.pdf\n",
      "\n",
      "Total documents loaded: 288\n",
      "\n",
      "Document breakdown by file type:\n",
      "- pdf: 251 documents\n",
      "- docx: 7 documents\n",
      "- v: 28 documents\n",
      "- lock: 2 documents\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import docx \n",
    "import docx2txt \n",
    "import PyPDF2\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    CSVLoader\n",
    ")\n",
    "\n",
    "from typing import Optional, Union, Dict, Set\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    JSONLoader\n",
    ")\n",
    "\n",
    "class UnsupportedFileTypeError(Exception):\n",
    "    \"\"\"Custom exception for unsupported file types.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_loader_class(\n",
    "    file_path: Union[str, Path],\n",
    "    strict_mode: bool = False\n",
    ") -> Optional[Union[TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader, UnstructuredMarkdownLoader, JSONLoader]]:\n",
    "    \"\"\"\n",
    "    Determine the appropriate document loader based on file extension.\n",
    "    \n",
    "    Args:\n",
    "        file_path (Union[str, Path]): Path to the file that needs to be loaded\n",
    "        strict_mode (bool): If True, raises an exception for unsupported file types\n",
    "                          If False, returns None for unsupported types\n",
    "    \n",
    "    Returns:\n",
    "        Optional[Union[...]]: Appropriate loader class for the file type, or None if unsupported\n",
    "        \n",
    "    Raises:\n",
    "        UnsupportedFileTypeError: If strict_mode is True and file type is unsupported\n",
    "        ValueError: If file_path is empty or invalid\n",
    "    \"\"\"\n",
    "    if not file_path:\n",
    "        raise ValueError(\"File path cannot be empty\")\n",
    "    \n",
    "    # Convert to Path object for more robust path handling\n",
    "    path = Path(file_path)\n",
    "    \n",
    "    # Get lowercase extension without the dot\n",
    "    try:\n",
    "        extension = path.suffix.lower().lstrip('.')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid file path format: {str(e)}\")\n",
    "    \n",
    "    # Define skip extensions\n",
    "    SKIP_EXTENSIONS: Set[str] = {\n",
    "        'zip', 'exe', 'bin', 'ppt', 'pptx', \n",
    "        'jpg', 'jpeg', 'png', 'gif', 'bmp',  # Skip binary image formats\n",
    "        'mp3', 'mp4', 'wav', 'avi',          # Skip media files\n",
    "        'db', 'sqlite', 'sqlite3'            # Skip database files\n",
    "    }\n",
    "    \n",
    "    # Define loader mapping\n",
    "    LOADER_MAP: Dict[str, type] = {\n",
    "        'pdf': PyPDFLoader,\n",
    "        'txt': TextLoader,\n",
    "        'text': TextLoader,\n",
    "        'doc': TextLoader,         # Basic text handling for .doc\n",
    "        'docx': Docx2txtLoader,\n",
    "        'csv': CSVLoader,\n",
    "        'md': UnstructuredMarkdownLoader,\n",
    "        'json': JSONLoader\n",
    "    }\n",
    "    \n",
    "    # Check if file type should be skipped\n",
    "    if extension in SKIP_EXTENSIONS:\n",
    "        if strict_mode:\n",
    "            raise UnsupportedFileTypeError(\n",
    "                f\"File type '.{extension}' is not supported for loading\"\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    # Get appropriate loader or default to TextLoader\n",
    "    loader_class = LOADER_MAP.get(extension)\n",
    "    \n",
    "    if loader_class is None and strict_mode:\n",
    "        raise UnsupportedFileTypeError(\n",
    "            f\"No specific loader found for '.{extension}' files\"\n",
    "        )\n",
    "    \n",
    "    return loader_class or TextLoader  # Default to TextLoader if no specific loader found\n",
    "\n",
    "def get_file_metadata(file_path: Union[str, Path]) -> dict:\n",
    "    \"\"\"\n",
    "    Get metadata about the file to be loaded.\n",
    "    \n",
    "    Args:\n",
    "        file_path (Union[str, Path]): Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing file metadata\n",
    "    \"\"\"\n",
    "    path = Path(file_path)\n",
    "    return {\n",
    "        'filename': path.name,\n",
    "        'extension': path.suffix.lower().lstrip('.'),\n",
    "        'size_bytes': path.stat().st_size if path.exists() else None,\n",
    "        'absolute_path': str(path.absolute()),\n",
    "        'exists': path.exists(),\n",
    "        'is_file': path.is_file() if path.exists() else None\n",
    "    }\n",
    "\n",
    "def load_documents(base_path: str) -> list:\n",
    "    \"\"\"Load documents from the specified path with appropriate loaders.\"\"\"\n",
    "    documents = []\n",
    "    text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Directory not found: {base_path}\")\n",
    "        return documents\n",
    "        \n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Get appropriate loader\n",
    "                loader_class = get_loader_class(file_path)\n",
    "                \n",
    "                # Skip if no loader is available for this file type\n",
    "                if loader_class is None:\n",
    "                    print(f\"⚠ Skipping unsupported file: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize loader with appropriate arguments\n",
    "                if loader_class == TextLoader:\n",
    "                    loader = loader_class(file_path, **text_loader_kwargs)\n",
    "                elif loader_class == CSVLoader:\n",
    "                    loader = loader_class(file_path, encoding='utf-8')\n",
    "                else:\n",
    "                    loader = loader_class(file_path)\n",
    "                \n",
    "                # Load document\n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Add metadata\n",
    "                doc_type = os.path.basename(root)\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update({\n",
    "                        \"doc_type\": doc_type,\n",
    "                        \"source\": file_path,\n",
    "                        \"file_type\": file_path.split('.')[-1],\n",
    "                        \"file_name\": file\n",
    "                    })\n",
    "                    documents.append(doc)\n",
    "                \n",
    "                print(f\"✓ Successfully loaded: {file_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error loading {file_path}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Usage\n",
    "base_path = \"../WILP_ASSIGNMENT_WORK\"\n",
    "documents = load_documents(base_path)\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "\n",
    "# Print document statistics\n",
    "if documents:\n",
    "    file_types = {}\n",
    "    for doc in documents:\n",
    "        file_type = doc.metadata.get('file_type', 'unknown')\n",
    "        file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nDocument breakdown by file type:\")\n",
    "    for file_type, count in file_types.items():\n",
    "        print(f\"- {file_type}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 393\n",
      "Document types found: {'.metadata', 'bitsfinalreportdocs', 'cpu_risc-master', 'uart_led_lab', 'WILP_ASSIGNMENT_WORK', 'WILP Final Dissertation'}\n"
     ]
    }
   ],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "# folders = glob.glob(\"Manuals/*\")\n",
    "\n",
    "# def add_metadata(doc, doc_type):\n",
    "#     doc.metadata[\"doc_type\"] = doc_type\n",
    "#     return doc\n",
    "\n",
    "# documents = []\n",
    "# for folder in folders:\n",
    "#     doc_type = os.path.basename(folder)\n",
    "#     loader = DirectoryLoader(folder, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "#     folder_docs = loader.load()\n",
    "#     documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 393 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "DB_NAME = \"vector_db\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(DB_NAME):\n",
    "    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run a quick test - should return a list of documents = 4\n",
    "question = \"Who is Samar Singh?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='b5e55bdf-17d3-49e8-ab1c-fdb2aebd2b97', metadata={'doc_type': 'WILP_ASSIGNMENT_WORK', 'file_name': '2020HT80616_RCC_Lab_Assignment_1.pdf', 'file_type': 'pdf', 'page': 2, 'source': '../WILP_ASSIGNMENT_WORK\\\\2020HT80616_RCC_Lab_Assignment_1.pdf'}, page_content='Name : Samar Singh Billawaria \\n                                                                                                                  BITS ID : 2020HT80616 \\n \\n3. N-BIT COUNTER CODE : \\n \\n \\n \\n4. CUSTOM DFF CODE :')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samar\\AppData\\Local\\Temp\\ipykernel_27496\\1192425351.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# create a new Chat with Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "MODEL = \"llama3.2:latest\"\n",
    "llm = ChatOllama(temperature=0.7, model=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samar Singh Billawaria is a student, with a BITS ID (Birla Institute of Technology and Science) of 2020HT80616.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple question\n",
    "\n",
    "query = \"Who is Samar ?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the  LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
